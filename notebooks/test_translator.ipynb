{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "# load \n",
    "from train import LatentTranslatorTrainer\n",
    "# %%\n",
    "trainer = LatentTranslatorTrainer.load_from_checkpoint('../models/latent_translator/dauntless-resonance-23/epoch=0-step=29795.ckpt')\n",
    "# %%\n",
    "trainer.model.bfloat16()\n",
    "trainer.model.cuda()\n",
    "trainer.model.eval()\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "vid = torch.load('/home/developer/workspace/data/tensors/valid/0000.pt')\n",
    "\n",
    "from lat2lat.models.wan_dcae import WANDCAEPair\n",
    "\n",
    "pairencoder = WANDCAEPair()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid.shape\n",
    "pairencoder.bfloat16()\n",
    "pairencoder.cuda()\n",
    "pairencoder.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = pairencoder.preprocess(vid[21:21+21].unsqueeze(0))\n",
    "with torch.no_grad():\n",
    "    za,zb = pairencoder(a.cuda(),b.cuda())\n",
    "print(za.shape, zb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "za_ = trainer.model.translate_batch(zb.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load wan decoder \n",
    "import sys\n",
    "sys.path.append('../owl-vaes')\n",
    "sys.path.append('../')\n",
    "import torch\n",
    "from diffusers import AutoencoderKLWan\n",
    "\n",
    "# Load the VAE\n",
    "vae = AutoencoderKLWan.from_pretrained(\n",
    "    \"Wan-AI/Wan2.1-VACE-14B-diffusers\",\n",
    "    subfolder=\"vae\",\n",
    "    torch_dtype=torch.bfloat16  # Optional: use half precision\n",
    ")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "vae = vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a boxplot of za_ \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.boxplot(za_.detach().cpu().float().flatten().numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lat2lat.utils import create_video_visualization, export_video_as_gif\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "with torch.no_grad():\n",
    "    zprep = za_.permute(0, 2, 1,3,4)\n",
    "    recon_vid = vae.decode(zprep.to(vae.device))\n",
    "    recon_vid = recon_vid.sample.permute(0, 2, 1,3,4)[0]\n",
    "    \n",
    "    # Also decode the original za for comparison\n",
    "    za_prep = za.permute(0, 2, 1,3,4)\n",
    "    za_vid = vae.decode(za_prep.to(vae.device))\n",
    "    za_vid = za_vid.sample.permute(0, 2, 1,3,4)[0]\n",
    "\n",
    "# Example usage with your reconstructed video\n",
    "print(\"Creating video visualizations...\")\n",
    "\n",
    "# 1. Animated playback - side by side comparison with synchronized playback\n",
    "print(\"\\n1. Creating synchronized side-by-side video comparison...\")\n",
    "\n",
    "# Create figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "fig.suptitle('Synchronized Video Comparison', fontsize=16)\n",
    "\n",
    "# Get video data\n",
    "_, video_np_za = create_video_visualization(za_vid, \"Original Video (za)\")\n",
    "_, video_np_recon = create_video_visualization(recon_vid, \"Reconstructed Video (za_)\")\n",
    "\n",
    "# Ensure both videos have the same number of frames\n",
    "min_frames = min(len(video_np_za), len(video_np_recon))\n",
    "video_np_za = video_np_za[:min_frames]\n",
    "video_np_recon = video_np_recon[:min_frames]\n",
    "\n",
    "# Initialize plots\n",
    "im1 = ax1.imshow(video_np_za[0])\n",
    "ax1.set_title('Original Video (za)')\n",
    "ax1.axis('off')\n",
    "\n",
    "im2 = ax2.imshow(video_np_recon[0])\n",
    "ax2.set_title('Reconstructed Video (za_)')\n",
    "ax2.axis('off')\n",
    "\n",
    "def animate(frame):\n",
    "    im1.set_array(video_np_za[frame])\n",
    "    im2.set_array(video_np_recon[frame])\n",
    "    return [im1, im2]\n",
    "\n",
    "# Create animation with synchronized playback\n",
    "anim = animation.FuncAnimation(\n",
    "    fig, animate, frames=min_frames, \n",
    "    interval=100, blit=True, repeat=True\n",
    ")\n",
    "\n",
    "# Display the synchronized animation\n",
    "display(HTML(anim.to_jshtml()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
